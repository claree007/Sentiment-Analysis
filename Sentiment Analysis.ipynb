{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Sentiment Analysis</h1>\n",
    "<hr/>\n",
    "\n",
    "<p>Using the IMDB dataset of movie reviews, predict whether the review is positive or negative.\n",
    "\n",
    "The dataset can be downloaded from <a href='http://ai.stanford.edu/%7Eamaas/data/sentiment/' target='_blank'>here</a>.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import nltk\n",
    "import pickle\n",
    "import gensim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Cleaning and tokenizing helper functions</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "replacements = \"\"\"aren't    are not\n",
    "can't   cannot\n",
    "couldn't    could not\n",
    "didn't  did not\n",
    "doesn't does not\n",
    "don't   do not\n",
    "hadn't  had not\n",
    "hasn't  has not\n",
    "haven't have not\n",
    "he'd    he would\n",
    "he'll   he will\n",
    "he's    he is\n",
    "i'd i would\n",
    "i'll    i will\n",
    "i'm i am\n",
    "i've    i have\n",
    "isn't   is not\n",
    "it's    it is, it has\n",
    "let's   let us\n",
    "mustn't must not\n",
    "shan't  shall not\n",
    "she'd   she would\n",
    "she'll  she will\n",
    "she's   she is\n",
    "shouldn't   should not\n",
    "that's  that is\n",
    "there's there is\n",
    "they'd  they would\n",
    "they'll they will\n",
    "they're they are\n",
    "they've they have\n",
    "we'd    we would\n",
    "we're   we are\n",
    "we've   we have\n",
    "weren't were not\n",
    "what'll what will\n",
    "what're what are\n",
    "what's  what is\n",
    "what've what have\n",
    "where's where is\n",
    "who'd   who would\n",
    "who'll  who will\n",
    "who're  who are\n",
    "who's   who is\n",
    "who've  who have\n",
    "won't   will not\n",
    "wouldn't    would not\n",
    "you'd   you would\n",
    "you'll  you will\n",
    "you're  you are\n",
    "you've  you have\"\"\"\n",
    "\n",
    "# expand words with apostrophe\n",
    "splitted = []\n",
    "for r in replacements.split('\\n'):\n",
    "    splitted.append(re.split(r'\\s', r, maxsplit=1))\n",
    "\n",
    "# load stopwords\n",
    "stopwords = []\n",
    "with open('stopwords.txt', 'r') as f:\n",
    "    stopwords = f.read().lower().split('\\n')\n",
    "\n",
    "def expand_sent(sent):\n",
    "    for split in splitted:\n",
    "        sent = re.sub(split[0], split[1], sent)\n",
    "    return sent\n",
    "\n",
    "def remove_stopwords(tokens):\n",
    "    new_tokens = []\n",
    "    for word in tokens:\n",
    "        if word not in stopwords:\n",
    "            new_tokens.append(word)\n",
    "    return new_tokens\n",
    "\n",
    "def clean_text(sent):\n",
    "    sent = expand_sent(sent.lower())\n",
    "    # removes 's eg: Amy's will become Amy\n",
    "    sent = re.sub(r\"'s\", \"\", sent)\n",
    "    # removes the hyphen from words joined together by it\n",
    "    sent = re.sub(r'(.*?)-(.*?)', r'\\1 \\2', sent)\n",
    "    # removes puntuations, extra characters and html tages\n",
    "    sent = re.sub(r'[\\'\"!@:.,?#*\\n()]|(<.*?>)', \" \", sent)\n",
    "    # removes more than 2 consecutive same characters with just 2\n",
    "    sent = re.sub(r'(.)\\1+', r'\\1\\1', sent)\n",
    "    # removes 2 or more spaces\n",
    "    sent = re.sub(r' +', \" \", sent)\n",
    "    return sent.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Load and create the dataset</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load the dataset\n",
    "dataset_path = './IMDB_dataset/aclimdb'\n",
    "reviews = []\n",
    "labels = []\n",
    "max_len = -1\n",
    "\n",
    "for s in ['train', 'test']:\n",
    "    pos_path = os.path.join(dataset_path, s, 'pos')\n",
    "    for file in tqdm(os.listdir(pos_path)):\n",
    "        with open(os.path.join(pos_path, file), 'r') as f:\n",
    "            text = remove_stopwords(nltk.word_tokenize(clean_text(f.read())))\n",
    "        if len(text) > max_len: max_len = len(text)\n",
    "        reviews.append(text)\n",
    "        labels.append(1)\n",
    "    \n",
    "    neg_path = os.path.join(dataset_path, s, 'neg')\n",
    "    for file in tqdm(os.listdir(neg_path)):\n",
    "        with open(os.path.join(neg_path, file), 'r') as f:\n",
    "            text = remove_stopwords(nltk.word_tokenize(clean_text(f.read())))\n",
    "        if len(text) > max_len: max_len = len(text)\n",
    "        reviews.append(text)\n",
    "        labels.append(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Load pretrained embeddings</h3>\n",
    "\n",
    "<p>Download the pretrained word2vec model from <a href='https://wikipedia2vec.github.io/wikipedia2vec/pretrained/' target='_blank'>here</a></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load the embedding\n",
    "embedding = gensim.models.KeyedVectors.load_word2vec_format('word2vec.txt', binary=False, limit=500000)\n",
    "\n",
    "print('Vocab size:', len(embedding.wv.vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# uncomment below code to save the limited embeddings to new file\n",
    "# embedding.save_word2vec_format('small_word2vec.txt', binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def word2idx(word):\n",
    "    return embedding.wv.vocab[word].index\n",
    "\n",
    "def idx2word(idx):\n",
    "    return embedding.wv.index2word[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Prepare the data for the ML model</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# pad the reviews to same length\n",
    "X = np.zeros([len(reviews), max_len], dtype=np.int32)\n",
    "y = np.array(labels, dtype=np.int32)\n",
    "\n",
    "for i, review in enumerate(tqdm(reviews)):\n",
    "    for t, word in enumerate(review):\n",
    "        X[i, t] = word2idx(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# split and shuffle the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=True)\n",
    "\n",
    "print(\"Train set size:\", len(X_train))\n",
    "print(\"Test set size:\", len(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Create a model</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_graphs(history, metric):\n",
    "    plt.plot(history.history[metric])\n",
    "    plt.plot(history.history['val_'+metric], '')\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(metric)\n",
    "    plt.legend([metric, 'val_'+metric])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load the embedding weights and get other values\n",
    "embedding_weights = embedding.wv.syn0\n",
    "vocab_size, embedding_size = embedding_weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define the model, loss and optimizer, and print the summary\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=embedding_size, weights=[embedding_weights]),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(100,  return_sequences=True)),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(100)),\n",
    "    tf.keras.layers.Dense(64, activation='tanh'),\n",
    "    tf.keras.layers.Dropout(0.25),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=False),\n",
    "              optimizer=tf.keras.optimizers.Adam(1e-3),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "history = model.fit(X_train, y_train, epochs=15, validation_data=[X_test, y_test], batch_size=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot_graphs(history, 'loss')\n",
    "plot_graphs(history, 'accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# run this file if you are satisfied with the model performance\n",
    "model.save('model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Test the model on new reviews</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# run the pipeline\n",
    "def predict_sentiment(text):\n",
    "    text = remove_stopwords(nltk.word_tokenize(clean_text(text)))\n",
    "    X = np.zeros([1, max_len], dtype=np.int32)\n",
    "    for i, word in enumerate(text):\n",
    "        X[0, i] = word2idx(word)\n",
    "    y_pred = model.predict(X)\n",
    "    print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "r = 'It is an awesome film. Kudos to the actors. Must watch!'\n",
    "predict_sentiment(r)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
